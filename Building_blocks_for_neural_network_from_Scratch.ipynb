{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations:\n",
    "Sigmoid:-\n",
    "         $$ \\frac{1}{1 + e^{-x}}$$\n",
    "Relu :- \n",
    "         $$\\max({0},{z})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    \"\"\"\n",
    "    Activations(their function and derivative) used:\n",
    "    sigmoid and \n",
    "    relu  \n",
    "    Reurns:\n",
    "    Acitvation output and their acitvation cache\"\"\"\n",
    "    \n",
    "    def sigmoid(Z):\n",
    "        \"\"\"\n",
    "        Calculates sigmoid function\n",
    "        Returns:\n",
    "        A: Output of sigmoid functiona\n",
    "        cache:saves the linear part of activation so as to use the info in calculation of backward prop\n",
    "        \"\"\"\n",
    "        e = np.exp(-Z)\n",
    "        A = 1/(1+e)\n",
    "        cache = Z\n",
    "        return A,cache\n",
    "    def relu(Z):\n",
    "        \"\"\"\n",
    "        Calculates relu function\n",
    "        Returns:\n",
    "        A: Output of relu function\n",
    "        cache:saves the linear part of activation so as to use the info in calculation of backward prop\n",
    "        \"\"\"\n",
    "        A = np.maximum(0,Z)\n",
    "        cache=Z\n",
    "        return A,cache\n",
    "    def sigmoid_backward(dA,cache):\n",
    "        \"\"\"\n",
    "        Calculates derivative of sigmoid function for backward prop\n",
    "        Returns:\n",
    "        dZ:Output of dervative of sigmoid function\n",
    "        \"\"\"\n",
    "        Z = cache\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA *s *s\n",
    "        return dZ\n",
    "    def relu_backwward(dA,cache):\n",
    "        \"\"\"\n",
    "        Calculates derivative of relu function for backward prop\n",
    "        Returns:\n",
    "        dZ:Output of derivative of relu function\n",
    "        \"\"\"\n",
    "        Z = cache\n",
    "        dZ = np.array(dA,copy=True)\n",
    "        dZ[Z<=0] = 0\n",
    "        return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialisation\n",
    "Default Intialisation :- Random initialisation of W(weight matrix) and zero initilisation of b(bias vector)\\\n",
    "He Inntialisation :- He Intialisation ,i.e., basically multiplying the default intialisation(both random) with $\\nu$.Here,$\\nu^2$ is\n",
    "                     $$\\nu^2 = \\frac{2}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def layer_parameter_initialise(layer_dimensions,initialiser = \"default\"):\n",
    "        \"\"\"\"\n",
    "        layers_dimensions = dimensions of layer\n",
    "        initialiser = Type of intialiser (He or default)\n",
    "        \n",
    "        Returns:\n",
    "        parameters = python dictionary(Initialised parameters (W,b))\n",
    "        W = Weight matrix \n",
    "        b = bias vector\n",
    "        \"\"\"        \n",
    "        parameters = {}\n",
    "        layers = len(layer_dimensions)\n",
    "        if initialiser=='He':\n",
    "            for i in range(1,layers):\n",
    "                parameters['W' + str(i)] = np.random.randn(layer_dimensions[i],layer_dimensions[i-1])*(np.sqrt(2/(i)))\n",
    "                parameters['b' + str(i)] = np.random.randn(layer_dimensions[i],1)*(np.sqrt(2/(i)))\n",
    "        else:\n",
    "            for i in range(1,layers):\n",
    "                parameters['W' + str(i)] = np.random.randn(layer_dimensions[i],layer_dimensions[i-1])*0.01\n",
    "                parameters['b' + str(i)] = np.zeros((layer_dimensions[i],1))\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_activation_function(Activations):\n",
    "    def linear_forward(A,W,b):\n",
    "        \"\"\"\n",
    "        A = acitivation from previous layer(previous layer,number of examples)\n",
    "        W = Weight matrix(size of current layer , size of previous layer)\n",
    "        b = bias vector (current layer,1)\n",
    "        Returns:\n",
    "        Z = Linear part of activation\n",
    "        cache = python tuple containing(A,W,b)\n",
    "        \"\"\"\n",
    "        Z = np.dot(W,A) + b\n",
    "        cache = (A,W,b)\n",
    "        return Z ,cache\n",
    "    def linear_forward_activations(A_p,W,b,activation):\n",
    "        \"\"\"\n",
    "        A = acitivation from previous layer\n",
    "        W = Weight matrix(size of current layer , size of previous layer)\n",
    "        b = bias vector (current layer,1)\n",
    "        activation = type of activations as string(sigmoid or relu)\n",
    "        Returns:\n",
    "        A = output of activation function\n",
    "        cache = python tuple containing linear_cahe and activation_cache for backward pass calculations\n",
    "        \"\"\"\n",
    "        if activation == 'sigmoid':\n",
    "            Z,linear_cache = Layer_activation_function.linear_forward(A_p,W,b)\n",
    "            A,activation_cache = Activations.sigmoid(Z)\n",
    "        elif activation == 'relu':\n",
    "            Z,linear_cache = Layer_activation_function.linear_forward(A_p,W,b)\n",
    "            A,activation_cache = Activations.relu(Z)\n",
    "        cache = (linear_cache,activation_cache)\n",
    "        return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X,parameters):\n",
    "    \"\"\"\n",
    "    Forward Prop with LINEAR -> RELU ----> LINEAR ->SIGMOID layer architecture\n",
    "    X : data(numpy array with (input_size,number_of_examples))\n",
    "    parameters: python dictionary ,containing output of layer_parameter_initialise function of Parameters class.\n",
    "    Returns:\n",
    "    Al : Output of last activation \n",
    "    cachces : a list of caches containing caches of linear_activations of class Layer_activation_function\n",
    "    \"\"\"\n",
    "    caches =[]\n",
    "    A = X\n",
    "    #number of layers \n",
    "    L = len(parameters)//2 \n",
    "    for i in range(1,L):\n",
    "        A ,cache = Layer_activation_function.linear_forward_activations(A,parameters['W' + str(i)], parameters['b' +str(i)],activation='relu')\n",
    "        caches.append(cache)\n",
    "    Al,cache = Layer_activation_function.linear_forward_activations(A , parameters['W'+str(L)],parameters['b'+str(L)],activation = 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return Al,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5,4)\n",
    "W1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "W2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "AL, caches = forward_pass(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost \n",
    "Cost function:-\n",
    "                $\\-y(\\log(p) + (1-y)(\\log(1-p)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    def compute_cost(Al,Y):\n",
    "        \"\"\"\n",
    "        Calculates the cost function\n",
    "        Al:\n",
    "        Y: label vector (1,number_of_examples)\n",
    "        Returns:\n",
    "        cost ---> cross-entropy cost\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]                  # number of examples\n",
    "        cost =  - (1/m) * (np.dot(Y,np.log(Al).T) + np.dot(1-Y,np.log(1-Al).T))\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y = np.asarray([[1, 1, 0]])\n",
    "AL = np.array([[.8,.9,0.4]])\n",
    "print(\"cost = \" + str(Cost.compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backward_Activations(Activations):\n",
    "    def linear_backward(dZ,cache):\n",
    "        \"\"\"\n",
    "        Linear part of the backward activation\n",
    "        dZ : Gradient of the cost w.r.t. output of current layer(layer l)\n",
    "        cache : tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "        Returns :\n",
    "        dA_prev --> gradient w.r.t acitvation(previous layer (l-1)) , same shape as A_prev\n",
    "        dW --> gradient w.r.t. W (current layer l),same shape as W\n",
    "        db -->gradient w.r.t. b(current layer l),same shape as b\n",
    "        \"\"\"\n",
    "        A_prev,W,b = cache\n",
    "        m = A_prev.shape[1]\n",
    "        dW = -(1/m)*np.dot(dZ,A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "        dA_prev = np.dot(W.T,dZ)\n",
    "        return dA_prev,dW,db\n",
    "    \n",
    "    def linear_backward_activations(dA ,cache,activation):\n",
    "        \"\"\"\n",
    "        Complete Activation function for backward prop\n",
    "        dA: post-activation gradient for current layer\n",
    "        cache:tuple containing linear cache and activation cache\n",
    "        activation:type of activation (sigmoid,relu)\n",
    "        Return:\n",
    "        dA_prev --> Gradient of the cost w.r.t. activation(previous layer l-1), same shape as A_prev\n",
    "        dW --> Gradient of the cost w.r.t. W (current layer l), same shape as W\n",
    "        db --> Gradient of the cost w.r.t. b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        linear_cache , activation_cache = cache\n",
    "        if activation == \"sigmoid\":\n",
    "            dZ = Activations.sigmoid_backward(dA,activation_cache)\n",
    "            dA_prev, dW, db = Backward_Activations.linear_backward(dZ,linear_cache)\n",
    "        elif activation==\"relu\":\n",
    "            dZ = Activations.relu_backward(dA,activation_cache)\n",
    "            dA_prev, dW, db = Backward_Activations.linear_backward(dZ,linear_cache)\n",
    "        return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(Al,Y,caches):\n",
    "    \"\"\"\n",
    "    Backward prop\n",
    "    Al:probability vector \n",
    "    Y: true label vector\n",
    "    caches:list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" and with \"sigmoid\"\n",
    "    Returns:\n",
    "    grads--> python dictionary, containing gradients of dA,dW,db\n",
    "    \"\"\"\n",
    "    grads ={}\n",
    "    L = len(caches)\n",
    "    m=Al.shape[1]\n",
    "    Y = Y.reshape(Al.shape)\n",
    "    dAl = -(np.divide(Y,Al) - np.divide(1-Y,1-Al))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = Backward_Activations.linear_backward_activations(dAl,current_cache,activation='sigmoid')\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = Backward_Activations.linear_backward_activations(dAl,current_cache,activation='sigmoid')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dA1': array([[ 0.2465359 , -0.30822344],\n",
      "       [-0.2706392 ,  0.3383578 ],\n",
      "       [ 0.03176322, -0.03971093]]), 'dW2': array([[ 0.42517377,  0.04556475, -0.15219449]]), 'db2': array([[0.03012452]]), 'dA0': array([[ 0.2465359 , -0.30822344],\n",
      "       [-0.2706392 ,  0.3383578 ],\n",
      "       [ 0.03176322, -0.03971093]]), 'dW1': array([[ 0.42517377,  0.04556475, -0.15219449]]), 'db1': array([[0.03012452]])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "AL = np.random.randn(1, 2)\n",
    "Y_assess = np.array([[1, 0]])\n",
    "\n",
    "A1 = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "Z1 = np.random.randn(3,2)\n",
    "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "A2 = np.random.randn(3,2)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "Z2 = np.random.randn(1,2)\n",
    "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "grads = backward_pass(AL, Y_assess, caches)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "Applying gradient descent:-\n",
    "$$\\frac{1}{N}\\sum_{i=0}^{i=N}(y - \\alpha * dy)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    parameters:python dictionary ,containing parameters \n",
    "    grads: python dictionary containing your gradients, output of backward_pass\n",
    "    \n",
    "    Returns:\n",
    "    parameters --> python dictionary containing your updated parameters(W,b)\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)] - learning_rate * grads['dW' + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)] - learning_rate * grads['db' + str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
